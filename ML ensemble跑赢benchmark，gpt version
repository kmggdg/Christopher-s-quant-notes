r"""
ML ensemble backtest using user's data cleaning and A-share helpers.
 - Keeps `read_excell_csmar`, `AShareSizer`, `AShareCommission` exactly as requested.
 - Builds a broad factor set, runs model training (RF + XGBoost stacked), selects important factors,
   predicts next-period returns, converts predictions to signals and backtests with Backtrader.
 - Produces performance metrics (Sharpe, IC) and visualizations saved to Desktop.

Usage example (Windows-friendly, use forward slashes or raw string to avoid escape issues):

    python ml_ensemble_backtest_backtrader.py --input C:/Users/MSI/Desktop/600030_5y.xlsx

Or in PowerShell (with explicit Python path):

    & C:/Users/MSI/AppData/Local/Programs/Python/Python313/python.exe C:/Users/MSI/Desktop/ml_ensemble_backtest_backtrader.py --input "C:/Users/MSI/Desktop/600030_5y.xlsx"

Notes:
 - Do NOT place plain backslash sequences like "C:\\Users\\MSI\\..." directly inside a Python source string literal
   (they may be interpreted as Unicode escapes such as \U). Use forward slashes or escape backslashes.

Dependencies:
    pandas, numpy, scikit-learn, xgboost, matplotlib, backtrader, scipy

Author: Generated by assistant; adapt hyperparams as needed.
"""

import argparse
import os
import math
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# ML
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import RidgeCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.inspection import permutation_importance

# Optional XGBoost
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except Exception:
    XGB_AVAILABLE = False

# Backtrader
import backtrader as bt

# Quant reporting (optional)
try:
    import quantstats as qs
    QS_AVAILABLE = True
except Exception:
    QS_AVAILABLE = False

# -------------------------
# 1) Keep user's cleaning + sizer + commission unchanged
# -------------------------

def read_excell_csmar(filepath):
    df = pd.read_excel(filepath)
    # 针对CSMAR数据的清洗
    if 0 in df.index: df.drop([0,1], inplace=True)
    if 'Stkcd' in df.columns: df.drop(columns=['Stkcd'], inplace=True)
    
    df['date'] = pd.to_datetime(df['Trddt'])
    df.set_index('date', inplace=True)
    df.drop(columns=['Trddt'], inplace=True)
    df.columns = ['open','high','low','close','volume']
    
    # 确保数据是数值型且按时间排序
    df = df.apply(pd.to_numeric, errors='coerce')
    df.sort_index(inplace=True)
    return df

class AShareSizer(bt.Sizer):
    params = (('percents', 0.95),) 
    def _getsizing(self, comminfo, cash, data, isbuy):
        if isbuy:
            price = data.close[0]
            target_cash = cash * self.params.percents
            if price == 0: return 0
            # A股核心逻辑：向下取整到100股 (1手)
            size = int(math.floor(target_cash / price / 100)) * 100
            return size
        return 0

class AShareCommission(bt.CommInfoBase):
    params = (('stocklike', True), ('commtype', bt.CommInfoBase.COMM_PERC), 
              ('percabs', True), ('stamp_duty', 0.001),)
    def _getcommission(self, size, price, pseudoexec):
        comm = size * price * self.p.commission
        # 卖出时额外收取印花税
        if size < 0: comm += abs(size) * price * self.p.stamp_duty
        return comm

# -------------------------
# 2) Feature engineering
# -------------------------

def build_factors(df):
    """Construct a wide factor matrix from OHLCV input. Returns dataframe with factor columns and NextReturn."""
    dd = df.copy()

    # basic returns
    dd['ret1'] = dd['close'].pct_change()
    dd['ret5'] = dd['close'].pct_change(5)

    # momentum features (short and medium term)
    dd['mom_5'] = dd['close'].pct_change(5)
    dd['mom_10'] = dd['close'].pct_change(10)
    dd['mom_20'] = dd['close'].pct_change(20)
    # longer-term momentum for trend-following
    dd['mom_60'] = dd['close'].pct_change(60)

    # volatility features
    dd['vol_10'] = dd['ret1'].rolling(10).std()
    dd['vol_20'] = dd['ret1'].rolling(20).std()
    dd['vol_60'] = dd['ret1'].rolling(60).std()

    # price range features
    dd['hl_range'] = (dd['high'] - dd['low']) / (dd['open'] + 1e-9)
    dd['body'] = (dd['close'] - dd['open']).abs() / (dd['open'] + 1e-9)

    # VWAP approx and divergence
    typ = (dd['high'] + dd['low'] + dd['close']) / 3
    dd['pv'] = typ * dd['volume']
    dd['vwap_20'] = dd['pv'].rolling(20).sum() / (dd['volume'].rolling(20).sum() + 1e-9)
    dd['vwap_dev'] = (dd['close'] - dd['vwap_20']) / (dd['vwap_20'] + 1e-9)

    # Moving averages and trend slope
    dd['ma10'] = dd['close'].rolling(10).mean()
    dd['ma50'] = dd['close'].rolling(50).mean()
    dd['ma120'] = dd['close'].rolling(120).mean()
    dd['ma10_50_diff'] = (dd['ma10'] - dd['ma50']) / (dd['ma50'] + 1e-9)
    dd['ma50_120_diff'] = (dd['ma50'] - dd['ma120']) / (dd['ma120'] + 1e-9)
    # distance to long MA (trend strength)
    dd['dist_ma120'] = (dd['close'] - dd['ma120']) / (dd['ma120'] + 1e-9)

    # Amihud illiquidity proxy
    dd['amt'] = dd['volume'] * dd['close']
    dd['amihud'] = (dd['ret1'].abs() / (dd['amt'] + 1e-9)).rolling(20).mean()

    # PV correlation (price change vs vol change)
    dd['pct_price'] = dd['close'].pct_change()
    dd['pct_vol'] = dd['volume'].pct_change()
    dd['pv_corr'] = dd['pct_price'].rolling(20).corr(dd['pct_vol'])

    # KER-like efficiency (Kaufman efficiency ratio proxy)
    change = (dd['close'] - dd['close'].shift(20)).abs()
    path = dd['close'].diff().abs().rolling(20).sum()
    dd['ker20'] = (change / (path + 1e-9)) * np.sign(dd['close'] - dd['close'].shift(20))

    # RSRS (slope of high~low) standardized
    N = 18
    cov_hl = dd['high'].rolling(N).cov(dd['low'])
    var_l = dd['low'].rolling(N).var()
    beta = cov_hl / (var_l + 1e-9)
    dd['rsrs'] = (beta - beta.rolling(600).mean()) / (beta.rolling(600).std() + 1e-9)

    # GK volatility (uses OHLC, more efficient)
    c1 = 0.5 * np.log(dd['high'] / dd['low'])**2
    c2 = (2 * np.log(2) - 1) * np.log(dd['close'] / dd['open'])**2
    dd['gk_vol'] = (c1 - c2).rolling(20).mean()

    # turnover proxy: vol / ma(vol)
    dd['vol_ma20'] = dd['volume'].rolling(20).mean()
    dd['turn_ratio'] = dd['volume'] / (dd['vol_ma20'] + 1e-9)

    # --- Technical indicators (pure pandas implementations) ---
    # ATR (14)
    high_low = dd['high'] - dd['low']
    high_prevclose = (dd['high'] - dd['close'].shift(1)).abs()
    low_prevclose = (dd['low'] - dd['close'].shift(1)).abs()
    tr = pd.concat([high_low, high_prevclose, low_prevclose], axis=1).max(axis=1)
    dd['atr14'] = tr.rolling(14).mean()

    # RSI (14)
    delta = dd['close'].diff()
    up = delta.clip(lower=0)
    down = -1 * delta.clip(upper=0)
    ma_up = up.rolling(14).mean()
    ma_down = down.rolling(14).mean()
    rs = ma_up / (ma_down + 1e-9)
    dd['rsi14'] = 100 - (100 / (1 + rs))

    # MACD (12,26,9)
    ema12 = dd['close'].ewm(span=12, adjust=False).mean()
    ema26 = dd['close'].ewm(span=26, adjust=False).mean()
    macd = ema12 - ema26
    signal = macd.ewm(span=9, adjust=False).mean()
    dd['macd'] = macd
    dd['macd_signal'] = signal
    dd['macd_hist'] = macd - signal

    # ADX (14) approximation
    plus_dm = dd['high'].diff()
    minus_dm = -dd['low'].diff()
    plus_dm = plus_dm.where((plus_dm > minus_dm) & (plus_dm > 0), 0.0)
    minus_dm = minus_dm.where((minus_dm > plus_dm) & (minus_dm > 0), 0.0)
    tr14 = tr.rolling(14).sum()
    plus_di = 100 * (plus_dm.rolling(14).sum() / (tr14 + 1e-9))
    minus_di = 100 * (minus_dm.rolling(14).sum() / (tr14 + 1e-9))
    dx = (abs(plus_di - minus_di) / (plus_di + minus_di + 1e-9)) * 100
    dd['adx14'] = dx.rolling(14).mean()

    # target: future 5-day return (trend target) -- better for trend-following strategies
    dd['target_next'] = dd['close'].pct_change(5).shift(-5)

    # dropna and select columns
    factor_cols = ['mom_5','mom_10','mom_20','mom_60','vol_10','vol_20','vol_60','hl_range','body',
                   'vwap_dev','amihud','pv_corr','ker20','rsrs','gk_vol','turn_ratio',
                   'ma10_50_diff','ma50_120_diff','dist_ma120', 'atr14','rsi14','macd_hist','adx14']

    out = dd[factor_cols + ['target_next']].dropna().copy()
    return out

# -------------------------
# 3) Modeling: ensemble and feature selection
# -------------------------

def train_models(df_factors, n_train_frac=0.8):
    """Train ensemble: RandomForest + XGBoost (if available) stacked with Ridge final estimator.
    Returns trained stacking model, feature list, train/test splits and predictions."""
    X = df_factors.drop(columns=['target_next'])
    y = df_factors['target_next']

    split = int(len(df_factors) * n_train_frac)
    X_train, X_test = X.iloc[:split], X.iloc[split:]
    y_train, y_test = y.iloc[:split], y.iloc[split:]

    # Base estimators
    estimators = []
    # Make RF slightly more sensitive (fewer min_samples_leaf) to capture start of trends
    rf = RandomForestRegressor(n_estimators=400, max_depth=8, min_samples_leaf=5, max_features='sqrt', n_jobs=-1, random_state=42)
    estimators.append(('rf', rf))

    if XGB_AVAILABLE:
        # XGBoost tuned modestly for trend signal
        xgbr = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=600, learning_rate=0.03, max_depth=5, subsample=0.85, colsample_bytree=0.8, random_state=42)
        estimators.append(('xgb', xgbr))

    # Stacking regressor - final estimator is small Ridge (adds stability)
    # passthrough=True allows meta-estimator to see original features, often improving stability
    final_est = RidgeCV(alphas=[0.01, 0.1, 1.0], fit_intercept=True)
    stack = StackingRegressor(estimators=estimators, final_estimator=final_est, n_jobs=-1, passthrough=True)

    print('Training ensemble (this may take a bit)...')
    stack.fit(X_train, y_train)

    # Predictions
    y_pred_train = pd.Series(stack.predict(X_train), index=X_train.index)
    y_pred_test = pd.Series(stack.predict(X_test), index=X_test.index)

    # smooth predictions to reduce noise for trend signals (rolling mean)
    y_pred_test_smooth = y_pred_test.rolling(window=3, min_periods=1).mean()
    y_pred_train_smooth = y_pred_train.rolling(window=3, min_periods=1).mean()

    # Permutation importance on test set for global factor importance
    print('Computing permutation importance (test set)...')
    try:
        perm_imp = permutation_importance(stack, X_test, y_test, n_repeats=30, random_state=42, n_jobs=-1)
        imp_df = pd.Series(perm_imp.importances_mean, index=X.columns).sort_values(ascending=False)
    except Exception as e:
        print('Permutation importance failed:', e)
        imp_df = pd.Series(stack.final_estimator_.coef_ if hasattr(stack.final_estimator_, 'coef_') else np.zeros(X.shape[1]), index=X.columns)
        imp_df = imp_df.abs().sort_values(ascending=False)

    return {
        'model': stack,
        'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test,
        'y_pred_train': y_pred_train_smooth, 'y_pred_test': y_pred_test_smooth,
        'feature_importance': imp_df
    }

# -------------------------
# 4) Backtrader integration
# -------------------------

class MLDataFeed(bt.feeds.PandasData):
    lines = ('ml_pred',)
    params = (('ml_pred', -1), )

class NetValueAnalyzer(bt.Analyzer):
    def create_analysis(self):
        self.rets = {}
    def next(self):
        self.rets[self.datas[0].datetime.datetime(0)] = self.strategy.broker.getvalue()

class MLStrategy(bt.Strategy):
    # continuous position sizing strategy:
    # - position weight is proportional to standardized prediction strength
    # - positions are volatility scaled (target vol) and limited to A-share lot sizes
    # - stop_loss_pct: exit if price falls more than this fraction from entry
    params = (('reverse', False), ('target_vol', 0.08), ('stop_loss_pct', 0.08), ('min_hold', 3), ('exit_consec', 3))

    def __init__(self):
        self.pred = self.datas[0].ml_pred
        self.entry_price = None
        self.hold_days = 0
        self.loss_streak = 0

    def log(self, txt, dt=None):
        dt = dt or self.datas[0].datetime.date(0)
        print(f'{dt.isoformat()} , {txt}')

    def _compute_weight(self, pv):
        # use global median and IQR (monkeypatched into class) to standardize prediction
        median = getattr(MLStrategy, '_pred_median', 0.0)
        iqr = getattr(MLStrategy, '_pred_iqr', 1.0)
        z = (pv - median) / (iqr + 1e-9)
        # positive signal only (long-only), compress with tanh to avoid extreme sizes
        w = np.tanh(max(z, 0))
        return float(w)

    def next(self):
        pv = self.pred[0]
        if self.params.reverse:
            pv = -pv

        price = self.data.close[0]
        # compute position weight and clamp
        w = self._compute_weight(pv)
        alloc_frac = min(max(w, 0.0), 1.0) * 0.9

        # volatility scaling: use conservative lower bound to avoid extreme leverage
        hist_vol = 0.02

        # compute desired cash allocation (use available cash as upper cap to avoid margin)
        cash = self.broker.get_cash()
        desired_cash = cash * alloc_frac * self.params.target_vol / (max(hist_vol, 1e-9))
        # cap desired cash to available cash to prevent margin / rejected orders
        desired_cash = min(desired_cash, cash)

        if not self.position:
            if alloc_frac > 0.01 and desired_cash > price * 100:  # at least one lot
                size = int(math.floor(desired_cash / price / 100.0)) * 100
                if size > 0:
                    self.buy(size=size)
        else:
            # update holding counters
            self.hold_days += 1
            try:
                today_ret = (price - self.data.close[-1]) / (self.data.close[-1] + 1e-9)
            except Exception:
                today_ret = 0.0

            if today_ret < 0:
                self.loss_streak += 1
            else:
                self.loss_streak = 0

            # exit rules: stop loss OR prediction collapse OR consecutive losses after min_hold
            if self.entry_price is not None and price < self.entry_price * (1 - self.params.stop_loss_pct):
                self.close()
            elif w < 0.01 and self.hold_days >= self.params.min_hold:
                self.close()
            elif self.loss_streak >= self.params.exit_consec and self.hold_days >= self.params.min_hold:
                self.close()

    def notify_order(self, order):
        if order.status in [order.Submitted, order.Accepted]:
            return
        if order.status in [order.Completed]:
            if order.isbuy():
                self.entry_price = order.executed.price
                self.hold_days = 0
                self.loss_streak = 0
                self.log(f'BUY EXECUTED, size={order.executed.size}, price={order.executed.price:.2f}, comm={order.executed.comm:.2f}')
            elif order.issell():
                self.log(f'SELL EXECUTED, size={order.executed.size}, price={order.executed.price:.2f}, comm={order.executed.comm:.2f}')
                self.entry_price = None
                self.hold_days = 0
                self.loss_streak = 0
        elif order.status in [order.Canceled, order.Margin, order.Rejected]:
            self.log('Order Canceled/Margin/Rejected')

# -------------------------
# 5) Orchestration: full pipeline and visualization
# -------------------------

def run_pipeline(excel_path, output_dir=None):
    if output_dir is None:
        output_dir = os.path.dirname(excel_path)

    # 1. prepare factors
    df = read_excell_csmar(excel_path)
    factors = build_factors(df)
    print('Factors shape:', factors.shape)

    # 2. train models
    res = train_models(factors)
    fi = res['feature_importance']
    print('\nTop factors by permutation importance:\n', fi.head(10))

    # 3. prepare backtest sample: use OOS period (last 20%)
    X_test = res['X_test']
    y_test = res['y_test']
    y_pred_test = res['y_pred_test']

    # compute IC (spearman) between pred and realized next-day return
    ic = stats.spearmanr(y_pred_test, y_test)[0]
    print(f'Rank IC (OOS): {ic:.4f}')

    # 4. build signal by ranking: use a lower threshold (top 67%) to increase trade frequency for trend capture
    q_thr = y_pred_test.quantile(0.67)
    signal = (y_pred_test > q_thr).astype(float)

    # 5. Backtrader: construct df_test with ml_pred column and backtest
    df_test = df.loc[X_test.index].copy()
    df_test['ml_pred'] = y_pred_test

    cerebro = bt.Cerebro(stdstats=False)
    cerebro.addstrategy(MLStrategy)
    datafeed = MLDataFeed(dataname=df_test)
    cerebro.adddata(datafeed)

    cerebro.broker.setcash(100000.0)
    cerebro.addsizer(AShareSizer, percents=0.95)
    cerebro.broker.addcommissioninfo(AShareCommission(commission=0.0005))

    # analyzer
    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='mysharpe', timeframe=bt.TimeFrame.Days, annualize=True, riskfreerate=0.0)
    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='mydrawdown')
    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='mytrade')
    cerebro.addanalyzer(NetValueAnalyzer, _name='nav')

    # attach thresholds into strategy instance before run: we'll use percentile thresholds based on OOS preds
    strat_kwargs = {}
    # find exit threshold (e.g., median) and set buy threshold to 67% quantile for more trades
    exit_thr = y_pred_test.quantile(0.5)

    # set global prediction stats on Strategy class for weight computation
    MLStrategy._pred_median = float(y_pred_test.median())
    MLStrategy._pred_iqr = float(y_pred_test.quantile(0.75) - y_pred_test.quantile(0.25))

    print('Initial cash:', cerebro.broker.getvalue())
    print('Running Backtest...')
    results = cerebro.run()
    strat = results[0]

    # 6. analyze results
    sharpe_info = strat.analyzers.mysharpe.get_analysis()
    dd_info = strat.analyzers.mydrawdown.get_analysis()
    trade_info = strat.analyzers.mytrade.get_analysis()

    print('\n=== Backtest Summary ===')
    print('Sharpe:', sharpe_info.get('sharperatio', None))
    print('Max Drawdown (%):', dd_info['max']['drawdown'])

    # extract nav series built by NetValueAnalyzer and align to df_test index
    nav_dict = strat.analyzers.nav.rets
    nav_series = pd.Series(nav_dict).sort_index()
    # reindex to trading dates (df_test.index) and forward-fill to ensure daily alignment
    nav_series = nav_series.reindex(df_test.index, method='ffill')
    strat_rets = nav_series.pct_change().dropna()
    bench_rets = df_test['close'].pct_change().dropna()
    common_idx = strat_rets.index.intersection(bench_rets.index)

    # 7. Save plots and tables
    # feature importance
    plt.figure(figsize=(8,4))
    fi.head(20).plot(kind='bar')
    plt.title('Permutation Importance (OOS test)')
    plt.tight_layout()
    fi_path = os.path.join(output_dir, 'factor_importance.png')
    plt.savefig(fi_path)
    plt.close()

    # cumulative returns
    plt.figure(figsize=(10,5))
    plt.plot((1+strat_rets).cumprod(), label='strategy')
    plt.plot((1+bench_rets.loc[common_idx]).cumprod(), label='benchmark')
    plt.legend(); plt.title('Cumulative returns')
    cum_path = os.path.join(output_dir, 'cumulative_returns.png')
    plt.savefig(cum_path)
    plt.close()

    # save metrics
    metrics = {
        'sharpe': sharpe_info.get('sharperatio', None),
        'max_drawdown_pct': dd_info['max']['drawdown'],
        'IC_oos': ic,
        'n_trades': trade_info.get('total', {}).get('total', 0)
    }
    metrics_df = pd.Series(metrics)
    metrics_df.to_csv(os.path.join(output_dir, 'backtest_metrics.csv'))

    # optional quantstats
    if QS_AVAILABLE:
        try:
            qs.reports.html(strat_rets.loc[common_idx], benchmark=bench_rets.loc[common_idx], output=os.path.join(output_dir, 'qs_report.html'))
        except Exception as e:
            print('QuantStats report failed:', e)

    print('Saved plots and metrics to', output_dir)
    return {
        'model': res['model'],
        'feature_importance': fi,
        'ic': ic,
        'nav': nav_series,
        'strat_rets': strat_rets,
        'bench_rets': bench_rets
    }

# -------------------------
# CLI
# -------------------------
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', '-i', required=True, help='Path to input xlsx (CSMAR-style)')
    parser.add_argument('--out', '-o', default=None, help='Output directory (default: same dir as input)')
    args = parser.parse_args()

    outdir = args.out if args.out else os.path.dirname(args.input)
    os.makedirs(outdir, exist_ok=True)

    result = run_pipeline(args.input, output_dir=outdir)
    print('\nDone. Outputs: factor_importance.png, cumulative_returns.png, backtest_metrics.csv (and optional qs_report.html)')
